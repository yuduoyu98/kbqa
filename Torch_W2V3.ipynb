{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 1801/1801 [00:00<?, ?it/s]\n",
      "Tokenizing documents: 100%|██████████| 1801/1801 [01:11<00:00, 25.36it/s]\n",
      "Processing questions: 100%|██████████| 530/530 [00:00<00:00, 6465.95it/s]\n",
      "Generating document vectors: 100%|██████████| 1801/1801 [02:34<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved to ./model/word2vec1800_1400_1000_10.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating accuracy: 100%|██████████| 530/530 [00:00<00:00, 530164.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7509433962264151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 加载停用词\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Vector_size = 1400\n",
    "Window_size = 1000\n",
    "Min_Count = 10\n",
    "DataNum = '1800'\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"加载 JSONL 文件并返回字典列表。\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"清洗文本，去掉 HTML 标签并转换为小写。\"\"\"\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # 去掉所有 HTML 标签\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 合并多个空格为一个空格\n",
    "    return text.lower()  # 转换为小写\n",
    "\n",
    "def segment_text(text):\n",
    "    \"\"\"对文本进行分词并去掉停用词。\"\"\"\n",
    "    cleaned_text = clean_text(text)\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "    \"\"\"将文档信息存储到字典中。\"\"\"\n",
    "    doc_dict = {}\n",
    "    for doc in tqdm(documents, desc=\"Processing documents\"):\n",
    "        doc_id = doc['document_id']\n",
    "        doc_text = doc['document_text']\n",
    "        doc_dict[doc_id] = doc_text\n",
    "    return doc_dict\n",
    "\n",
    "def get_vector_mean(model, words):\n",
    "    \"\"\"获取词向量的均值，忽略不在词汇表中的词。\"\"\"\n",
    "    valid_words = [word for word in words if word in model.wv]\n",
    "    if not valid_words:\n",
    "        return None  # 如果没有有效词，则返回 None\n",
    "    return torch.mean(torch.tensor(model.wv[valid_words]), dim=0).unsqueeze(0)\n",
    "\n",
    "def preprocess_questions(questions, model):\n",
    "    \"\"\"将问题、答案和参考文档ID存储到字典中，同时生成问题向量。\"\"\"\n",
    "    question_dict = {}\n",
    "    for question in tqdm(questions, desc=\"Processing questions\"):\n",
    "        question_text = question['question']\n",
    "        answer_text = question['answer']\n",
    "        reference_doc_ids = question.get('document_id', [])\n",
    "        \n",
    "        # 计算问题向量\n",
    "        question_vector = get_vector_mean(model, segment_text(question_text))\n",
    "        \n",
    "        question_dict[question_text] = {\n",
    "            'answer': answer_text,\n",
    "            'document_id': reference_doc_ids,\n",
    "            'vector': question_vector  # 存储问题向量\n",
    "        }\n",
    "    return question_dict\n",
    "\n",
    "def train_word2vec(documents):\n",
    "    \"\"\"训练 Word2Vec 模型，并显示进度。\"\"\"\n",
    "    tokenized_docs = []\n",
    "    for doc in tqdm(documents.values(), desc=\"Tokenizing documents\"):\n",
    "        tokenized_docs.append(segment_text(doc))\n",
    "    \n",
    "    model = Word2Vec(sentences=tokenized_docs, vector_size=Vector_size, window=Window_size, min_count=Min_Count, workers=32)\n",
    "    return model\n",
    "\n",
    "def Candidate_Calculation(question_vector, doc_dict):\n",
    "    # 将文档向量转换为 NumPy 数组\n",
    "    doc_vectors_array = torch.stack(list(doc_vectors.values())).numpy()\n",
    "    doc_vectors_array = doc_vectors_array.squeeze()  # 去掉多余的维度\n",
    "\n",
    "    # for question_text, question_data in tqdm(question_vectors.items(), desc=\"Validating accuracy\"):\n",
    "    #     question_vector = question_data['vector'].detach().numpy()  # 获取问题向量\n",
    "\n",
    "    # 计算相似度\n",
    "    similarities = cosine_similarity(question_vector, doc_vectors_array)\n",
    "    \n",
    "    # 找到最匹配的五个文档\n",
    "    top_5_indices = similarities.argsort()[0][-5:][::-1]\n",
    "    \n",
    "    # 根据相对index找到doc_id\n",
    "    top_5_doc_ids = [list(doc_dict.keys())[i] for i in top_5_indices]\n",
    "    \n",
    "    # 输出结果\n",
    "    return top_5_doc_ids\n",
    "\n",
    "def validate_accuracy(question_dict):\n",
    "    # 判断 ['document_id_top5'] 是否包含 ['document_id_answer']\n",
    "    correct = 0\n",
    "    for question_text, question_data in tqdm(question_dict.items(), desc=\"Validating accuracy\"):\n",
    "        if question_data['document_id_answer'] in question_data['document_id_top5']:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(question_dict) if question_dict else 0\n",
    "    return accuracy\n",
    "\n",
    "# 主程序\n",
    "documents = load_jsonl('./data/documents'+DataNum+'.jsonl')\n",
    "questions = load_jsonl('./data/train'+DataNum+'.jsonl')\n",
    "\n",
    "# 预处理文档和问题\n",
    "doc_dict = preprocess_documents(documents)\n",
    "model = train_word2vec(doc_dict)\n",
    "question_dict = preprocess_questions(questions, model)\n",
    "\n",
    "# 生成文档向量\n",
    "doc_vectors = {}\n",
    "for doc_id, doc_text in tqdm(doc_dict.items(), desc=\"Generating document vectors\"):\n",
    "    doc_vec_mean = get_vector_mean(model, segment_text(doc_text))\n",
    "    if doc_vec_mean is not None:\n",
    "        doc_vectors[doc_id] = doc_vec_mean\n",
    "\n",
    "save_path = './model/word2vec'+ str(DataNum) + '_' + str(Vector_size) + '_' + str(Window_size) + '_' + str(Min_Count)+'.model'\n",
    "model.save(save_path)\n",
    "print(f\"Word2Vec model saved to {save_path}\")\n",
    "save_doc_vector_path = './data/doc_vector'+ str(DataNum) + '_' + str(Vector_size) + '_' + str(Window_size) + '_' + str(Min_Count) + '.jsonl'\n",
    "save_ques_vector_path = './data/ques_vector'+ str(DataNum) + '_' + str(Vector_size) + '_' + str(Window_size) + '_' + str(Min_Count) + '.jsonl'\n",
    "\n",
    "# 保存文档向量到 JSONL\n",
    "with open(save_doc_vector_path, 'w', encoding='utf-8') as doc_file:\n",
    "    for doc_id, doc_text in doc_dict.items():\n",
    "        doc_vector = doc_vectors.get(doc_id).detach().numpy().tolist() if doc_id in doc_vectors else None\n",
    "        doc_entry = {\n",
    "            'document_id': doc_id,\n",
    "            'document_text': doc_text,\n",
    "            'document_vector': doc_vector\n",
    "        }\n",
    "        doc_file.write(json.dumps(doc_entry) + '\\n')\n",
    "\n",
    "# 保存问题向量到 JSONL，并将整个字典保存到变量 val_dict\n",
    "# dictionary \n",
    "val_dict = {}\n",
    "with open(save_ques_vector_path, 'w', encoding='utf-8') as ques_file:\n",
    "    for question_text, question_data in question_dict.items():\n",
    "        ques_entry = {\n",
    "            'question_text': question_text,\n",
    "            'question_answer': question_data['answer'],\n",
    "            'document_id_answer': question_data['document_id'],\n",
    "            'document_id_top5': Candidate_Calculation(question_data['vector'].detach().numpy(), doc_dict),\n",
    "            'question_vector': question_data['vector'].detach().numpy().tolist() if question_data['vector'] is not None else None\n",
    "        }\n",
    "        val_dict[question_text] = ques_entry\n",
    "        ques_file.write(json.dumps(ques_entry) + '\\n')\n",
    "\n",
    "# 验证准确性\n",
    "print(validate_accuracy(val_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
