{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Load stop words\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "# read words from abbreviations.txt\n",
    "with open('abbreviations.txt', 'r') as file:\n",
    "    abbreviations = file.read().splitlines()\n",
    "\n",
    "DataNum = ''\n",
    "model_dir = './model/TF_IDF'\n",
    "SaveDir = './data/TF_IDF_annotated/'\n",
    "CHUNK_SIZE = 1000\n",
    "Debug = 0\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load a JSONL file and return a list of dictionaries.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def load_dict(file_path):\n",
    "    \"\"\"Load a dictionary from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing HTML tags, URLs, and converting to lowercase.\"\"\"\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'//[^\\s]*', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def process_text(text):\n",
    "    def is_valid_amount(word):\n",
    "        # 支持负数和小数的金额格式\n",
    "        if word.startswith('$') or word.startswith('-$'):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def format_amount(word):\n",
    "        # 去掉$符号和逗号\n",
    "        cleaned_amount = word.replace('$', '').replace(',', '')\n",
    "        return f\"(Amount${cleaned_amount})\"\n",
    "\n",
    "    def is_valid_date(word):\n",
    "        # 检查年份格式\n",
    "        if word.isdigit() and len(word)==4 and 1400 <= int(word) <= 2150:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def format_date(word):\n",
    "        # 格式化年份\n",
    "        return f\"([Date]{word})\"\n",
    "\n",
    "    def is_valid_time(word):\n",
    "        # 检查时间格式\n",
    "        if word[-3:] == 'p.m' or word[-3:] == 'a.m':\n",
    "            word = word[:-3]\n",
    "        parts = word.split(':')\n",
    "        # 支持时:分 或 时:分:秒\n",
    "        if len(parts) == 2 or len(parts) == 3:\n",
    "            hours, minutes = parts[0], parts[1]\n",
    "            # 确保小时和分钟都是数字\n",
    "            if hours.isdigit() and minutes.isdigit():\n",
    "                if (0 <= int(hours) <= 23) and (0 <= int(minutes) <= 59):\n",
    "                    # 如果有秒，检查秒的有效性\n",
    "                    if len(parts) == 3:\n",
    "                        seconds = parts[2]\n",
    "                        if seconds.isdigit() and 0 <= int(seconds) <= 59:\n",
    "                            return True\n",
    "                    else:\n",
    "                        return True\n",
    "        return False\n",
    "\n",
    "    def format_time(word):\n",
    "        # 格式化时间\n",
    "        hours = minutes = seconds = 0\n",
    "        parts = word.split(':')\n",
    "        flag = -1\n",
    "        if word[-3:] == 'p.m':\n",
    "            word = word[:-3]\n",
    "            flag = 1\n",
    "        elif word[-3:] == 'a.m':\n",
    "            word = word[:-3]\n",
    "            flag = 0\n",
    "        if len(parts) == 3:\n",
    "            hours, minutes, seconds = parts\n",
    "            str = f\"([Time]{hours}h{minutes}m{seconds}s)\"\n",
    "        elif len(parts) == 2:\n",
    "            hours, minutes = parts\n",
    "            str = f\"([Time]{hours}h{minutes}m0s)\"\n",
    "        if flag == 1:\n",
    "            str = str[:-1]\n",
    "            str += 'p.m)'\n",
    "        else:\n",
    "            str = str[:-1]\n",
    "            str += 'a.m)'\n",
    "        return str\n",
    "    def is_valid_temperature(word):\n",
    "        # 检查温度格式\n",
    "        if word.startswith('-'):\n",
    "            word = word[1:]\n",
    "        if word[:-2].replace('.', '').isdigit() and word[-1] in ['C', 'F', 'c', 'f']:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def format_temperature(word):\n",
    "        # 格式化温度\n",
    "        return f\"([Temperature]{word})\"\n",
    "\n",
    "    def is_valid_abbreviation(word):\n",
    "        # 检查常见缩写\n",
    "        # 使用 WordNet 查找缩写\n",
    "        if word in abbreviations:\n",
    "            return True\n",
    "\n",
    "    def format_abbreviation(word):\n",
    "        # 格式化缩写\n",
    "        return f\"([Abbreviation]{word})\"  \n",
    "    \n",
    "\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        # remove punctuation  from word's ending if it is there\n",
    "        word = word.rstrip(string.punctuation)\n",
    "        if is_valid_amount(word):\n",
    "            words[i] = format_amount(word)\n",
    "        elif is_valid_date(word):\n",
    "            words[i] = format_date(word)\n",
    "        elif is_valid_time(word):\n",
    "            words[i] = format_time(word)\n",
    "        elif is_valid_temperature(word):\n",
    "            words[i] = format_temperature(word)\n",
    "        # elif is_valid_abbreviation(word):\n",
    "        #     words[i] = format_abbreviation(word)\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def segment_text(text):\n",
    "    \"\"\"Tokenize text while preserving key information like amounts, dates, etc.\"\"\"\n",
    "    text = process_text(text)\n",
    "    # 定义特殊模式\n",
    "    special_terms = ['(Amount$', '([Date]', '([Time]', '([Temperature]', '([Abbreviation]']\n",
    "\n",
    "    # 分词并处理特殊模式\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        # 检查是否为特殊模式的开始\n",
    "        if any(text.startswith(term, i) for term in special_terms):\n",
    "            # 找到特殊模式的结束位置\n",
    "            end = text.find(')', i) + 1\n",
    "            if end > i:  # 如果找到了结束位置\n",
    "                tokens.append(text[i:end])  # 将特殊模式添加到 tokens\n",
    "                i = end  # 更新索引\n",
    "            else:\n",
    "                tokens.append(text[i])  # 如果没有找到，添加当前字符\n",
    "                i += 1\n",
    "        else:\n",
    "            # 正常分词\n",
    "            word = ''\n",
    "            while i < len(text) and text[i] not in [' ', '(', ')']:\n",
    "                #debug\n",
    "                # print(text[i])\n",
    "                word += text[i]\n",
    "                i += 1\n",
    "            if word not in [',','.','?','!',';','(',')','[',']','{','}','-','_','+','=','*','/','\\\\','|','&','^','%','$','#','@','!','~','`',':','<','>','\"','\\'']:\n",
    "                tokens.append(word)\n",
    "            # 跳过空格\n",
    "            if word == '':\n",
    "                i += 1\n",
    "            while i < len(text) and text[i] == ' ':\n",
    "                i += 1\n",
    "    # 移除停用词\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "    \"\"\"Store document information in a dictionary.\"\"\"\n",
    "    doc_dict = {}\n",
    "    for doc in tqdm(documents, desc=\"Processing documents\"):\n",
    "        doc_id = doc['document_id']\n",
    "        doc_text = clean_text(doc['document_text'])\n",
    "        doc_dict[doc_id] = doc_text\n",
    "    return doc_dict\n",
    "\n",
    "def preprocess_questions(questions, question_tfidf_matrix):\n",
    "    \"\"\"Store questions and their information in a dictionary.\"\"\"\n",
    "    question_dict = {}\n",
    "    for idx, question in tqdm(enumerate(questions), desc=\"Processing questions\"):\n",
    "        question_text = question['question']\n",
    "        answer_text = question['answer']\n",
    "        reference_doc_ids = question.get('document_id', [])\n",
    "        question_dict[question_text] = {\n",
    "            'answer': answer_text,\n",
    "            'document_id': reference_doc_ids,\n",
    "            'vector': question_tfidf_matrix[idx]\n",
    "        }\n",
    "    return question_dict\n",
    "\n",
    "# def compute_tfidf(text, vectorizer):\n",
    "#     \"\"\"Compute TF-IDF values.\"\"\"\n",
    "#     tfidf_matrix = vectorizer.fit_transform(text)\n",
    "#     return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "def compute_tfidf(text, vectorizer):\n",
    "    \"\"\"Compute TF-IDF values, ensuring special tags have IDF of 1.\"\"\"\n",
    "    tfidf_matrix = vectorizer.fit_transform(text)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Set IDF for special tags to 1\n",
    "    for idx, feature in enumerate(feature_names):\n",
    "        if feature.startswith('(Amount') or feature.startswith('([Date]') or \\\n",
    "           feature.startswith('([Time]') or feature.startswith('([Temperature]') or \\\n",
    "           feature.startswith('([Abbreviation]'):\n",
    "            vectorizer.idf_[idx] = 1.0  # Set IDF to 1 for special terms\n",
    "\n",
    "    return tfidf_matrix, feature_names\n",
    "\n",
    "def Candidate_Calculation(question_vector, doc_vectors):\n",
    "    \"\"\"Calculate the top 5 matching documents.\"\"\"\n",
    "    question_vector = question_vector.reshape(1, -1)\n",
    "    similarities = cosine_similarity(question_vector, doc_vectors)\n",
    "    top_5_indices = similarities.argsort()[0][-5:][::-1]\n",
    "    return top_5_indices\n",
    "\n",
    "def validate_accuracy(question_dict, doc_dict, doc_vectors):\n",
    "    \"\"\"Validate accuracy: Check if candidate documents contain reference document IDs.\"\"\"\n",
    "    correct = 0\n",
    "    total = len(question_dict)\n",
    "    \n",
    "    for question_text, question_data in tqdm(question_dict.items(), desc=\"Validating accuracy\"):\n",
    "        question_vector = question_data['vector']\n",
    "        top_5_indices = Candidate_Calculation(question_vector.toarray(), doc_vectors)\n",
    "        top_5_doc_ids = [list(doc_dict.keys())[i] for i in top_5_indices]\n",
    "        \n",
    "        reference_doc_ids = question_data['document_id']\n",
    "        if isinstance(reference_doc_ids, list):\n",
    "            if any(doc_id in top_5_doc_ids for doc_id in reference_doc_ids):\n",
    "                correct += 1\n",
    "        else:\n",
    "            if reference_doc_ids in top_5_doc_ids:\n",
    "                correct += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def save_sparse_vector(vector, file_path):\n",
    "    \"\"\"Save a sparse vector in a compact format.\"\"\"\n",
    "    row, col = vector.nonzero()\n",
    "    data = vector.data\n",
    "    entry = {\n",
    "        'indices': col.tolist(),\n",
    "        'values': data.tolist(),\n",
    "        'shape': vector.shape,\n",
    "        'indptr': vector.indptr.tolist()\n",
    "    }\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(entry, f)\n",
    "\n",
    "def save_dict_to_json(data_dict, file_path):\n",
    "    \"\"\"Save a dictionary to a JSON file.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_dict, f)\n",
    "\n",
    "# Main program\n",
    "documents = load_jsonl(f'./data/documents{DataNum}.jsonl')\n",
    "questions = load_jsonl(f'./data/train{DataNum}.jsonl')\n",
    "\n",
    "# Preprocess documents and questions\n",
    "doc_dict = preprocess_documents(documents)\n",
    "vectorizer = TfidfVectorizer(tokenizer=segment_text, max_df=0.95)\n",
    "all_text = list(doc_dict.values()) + [q['question'] for q in questions]\n",
    "tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "\n",
    "# Split TF-IDF matrix into documents and questions\n",
    "num_doc = len(doc_dict)\n",
    "doc_tfidf_matrix = tfidf_matrix[:num_doc]\n",
    "question_tfidf_matrix = tfidf_matrix[num_doc:]\n",
    "\n",
    "# Process questions to include TF-IDF vectors\n",
    "question_dict = preprocess_questions(questions, question_tfidf_matrix)\n",
    "\n",
    "# Create directory for vector files if it doesn't exist\n",
    "vector_dir = f'{SaveDir}vector{DataNum}'\n",
    "os.makedirs(vector_dir, exist_ok=True)\n",
    "\n",
    "# Save document vectors to JSONL\n",
    "save_doc_vector_path = f'{SaveDir}doc_vector{DataNum}.jsonl'\n",
    "with open(save_doc_vector_path, 'w', encoding='utf-8') as doc_file:\n",
    "    for i, (doc_id, doc_text) in enumerate(doc_dict.items()):\n",
    "        doc_vector = doc_tfidf_matrix[i]\n",
    "        vector_file_path = f'{vector_dir}/vector_{doc_id}.json'\n",
    "        save_sparse_vector(doc_vector, vector_file_path)\n",
    "        doc_entry = {\n",
    "            'document_id': doc_id,\n",
    "            'document_text': doc_text,\n",
    "            'document_vector_path': vector_file_path\n",
    "        }\n",
    "        doc_file.write(json.dumps(doc_entry) + '\\n')\n",
    "\n",
    "# Save question vectors to JSONL\n",
    "save_ques_vector_path = f'{SaveDir}ques_vector{DataNum}.jsonl'\n",
    "with open(save_ques_vector_path, 'w', encoding='utf-8') as ques_file:\n",
    "    for question_text, question_data in question_dict.items():\n",
    "        vector_file_path = f'{vector_dir}/vector_ques{hash(question_text)}.json'\n",
    "        save_sparse_vector(question_data['vector'], vector_file_path)\n",
    "        ques_entry = {\n",
    "            'question_text': question_text,\n",
    "            'question_answer': question_data['answer'],\n",
    "            'document_id_answer': question_data['document_id'],\n",
    "            'question_vector_path': vector_file_path\n",
    "        }\n",
    "        ques_file.write(json.dumps(ques_entry) + '\\n')\n",
    "\n",
    "# Create a dictionary for TF-IDF feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_feature_dict = {i: feature for i, feature in enumerate(feature_names)}\n",
    "save_dict_to_json(tfidf_feature_dict, f'{model_dir}/dict_{DataNum}.json')\n",
    "\n",
    "# Validate accuracy\n",
    "accuracy = validate_accuracy(question_dict, doc_dict, doc_tfidf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Reconstruction accuracy validation\n",
    "def reconstruct_text(sparse_vector, feature_dict, top_n=100):\n",
    "    \"\"\"Reconstruct text from a sparse vector using a feature dictionary.\"\"\"\n",
    "    indices = sparse_vector['indices']\n",
    "    values = sparse_vector['values']\n",
    "    index_to_word = {index: feature_dict[str(index)] for index in indices}\n",
    "    sorted_indices = np.argsort(values)[-top_n:][::-1]\n",
    "    reconstructed_words = [index_to_word[indices[i]] for i in sorted_indices]\n",
    "    return ' '.join(reconstructed_words)\n",
    "\n",
    "def calculate_reconstruction_accuracy(doc_dict, feature_dict, top_n=100):\n",
    "    \"\"\"Calculate the accuracy of text reconstruction.\"\"\"\n",
    "    correct = 0\n",
    "    total = len(doc_dict)\n",
    "\n",
    "    for i, (doc_id, original_text) in enumerate(doc_dict.items()):\n",
    "        sparse_vector_path = f'{SaveDir}vector{DataNum}/vector_{doc_id}.json'\n",
    "        sparse_vector = load_dict(sparse_vector_path)\n",
    "        reconstructed_text = reconstruct_text(sparse_vector, feature_dict, top_n)\n",
    "\n",
    "        original_words = set(original_text.split())\n",
    "        reconstructed_words = set(reconstructed_text.split())\n",
    "        original_words = original_words.difference(stop_words)\n",
    "\n",
    "        common_words = original_words.intersection(reconstructed_words)\n",
    "        accuracy = len(common_words) / min(len(original_words), len(reconstructed_words)) if min(len(original_words), len(reconstructed_words)) > 0 else 0\n",
    "        \n",
    "        if accuracy > 0.98:  # Threshold for considering it a \"match\"\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(f\"Doc ID: {doc_id}\")\n",
    "            print(original_words.difference(common_words))\n",
    "            print(reconstructed_words.difference(common_words))\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "            print()\n",
    "\n",
    "    print(f\"Correct: {correct}\", f\"Total: {total}\")\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Load the necessary data for reconstruction accuracy\n",
    "doc_vector_data = load_jsonl(f'{SaveDir}doc_vector{DataNum}.jsonl')\n",
    "doc_dict = {doc['document_id']: doc['document_text'] for doc in doc_vector_data}\n",
    "\n",
    "tfidf_feature_dict = load_dict(f'{model_dir}/dict_{DataNum}.json')\n",
    "# Calculate reconstruction accuracy\n",
    "reconstruction_accuracy = calculate_reconstruction_accuracy(doc_dict, tfidf_feature_dict)\n",
    "print(\"Reconstruction Accuracy:\", reconstruction_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
