{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 12138/12138 [00:00<00:00, 1517133.89it/s]\n",
      "g:\\Anaconda\\Miniconda\\envs\\MyNLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Processing questions: 8000it [00:00, 13985.96it/s]\n",
      "Validating accuracy: 100%|██████████| 8000/8000 [22:34<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os  # Make sure to import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "DataNum = ''\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file and return a list of dictionaries.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing HTML tags and converting to lowercase.\"\"\"\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Merge multiple spaces\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "def segment_text(text):\n",
    "    \"\"\"Tokenize text and remove stop words.\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "    \"\"\"Store document information in a dictionary.\"\"\"\n",
    "    doc_dict = {}\n",
    "    for doc in tqdm(documents, desc=\"Processing documents\"):\n",
    "        doc_id = doc['document_id']\n",
    "        doc_text = doc['document_text']\n",
    "        doc_dict[doc_id] = doc_text\n",
    "    return doc_dict\n",
    "\n",
    "def preprocess_questions(questions, question_tfidf_matrix):\n",
    "    \"\"\"Store questions, answers, and reference document IDs in a dictionary.\"\"\"\n",
    "    question_dict = {}\n",
    "    for idx, question in tqdm(enumerate(questions), desc=\"Processing questions\"):\n",
    "        question_text = question['question']\n",
    "        answer_text = question['answer']\n",
    "        reference_doc_ids = question.get('document_id', [])\n",
    "        \n",
    "        # Store the sparse matrix directly without converting to dense\n",
    "        question_dict[question_text] = {\n",
    "            'answer': answer_text,\n",
    "            'document_id': reference_doc_ids,  # Store reference document IDs\n",
    "            'vector': question_tfidf_matrix[idx]  # Keep as sparse matrix\n",
    "        }\n",
    "    return question_dict\n",
    "\n",
    "def compute_tfidf(text, vectorizer):\n",
    "    \"\"\"Compute TF-IDF values.\"\"\"\n",
    "    tfidf_matrix = vectorizer.fit_transform(text)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "def Candidate_Calculation(question_vector, doc_vectors):\n",
    "    \"\"\"Calculate the top 5 matching documents.\"\"\"\n",
    "    question_vector = question_vector.reshape(1, -1)  # Ensure it's a 2D array\n",
    "    similarities = cosine_similarity(question_vector, doc_vectors)\n",
    "    top_5_indices = similarities.argsort()[0][-5:][::-1]\n",
    "    return top_5_indices\n",
    "\n",
    "def validate_accuracy(question_dict, doc_dict, doc_vectors):\n",
    "    \"\"\"Validate accuracy: Check if candidate documents contain reference document IDs.\"\"\"\n",
    "    correct = 0\n",
    "    total = len(question_dict)\n",
    "    \n",
    "    for question_text, question_data in tqdm(question_dict.items(), desc=\"Validating accuracy\"):\n",
    "        question_vector = question_data['vector']  # Sparse matrix\n",
    "        top_5_indices = Candidate_Calculation(question_vector.toarray(), doc_vectors)\n",
    "        top_5_doc_ids = [list(doc_dict.keys())[i] for i in top_5_indices]\n",
    "        \n",
    "        reference_doc_ids = question_data['document_id']\n",
    "        if isinstance(reference_doc_ids, list):\n",
    "            if any(doc_id in top_5_doc_ids for doc_id in reference_doc_ids):\n",
    "                correct += 1\n",
    "        else:\n",
    "            if reference_doc_ids in top_5_doc_ids:\n",
    "                correct += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def save_sparse_vector(vector, file_path):\n",
    "    \"\"\"Save a sparse vector in a compact format.\"\"\"\n",
    "    row, col = vector.nonzero()\n",
    "    data = vector.data\n",
    "    entry = {\n",
    "        'indices': col.tolist(),\n",
    "        'values': data.tolist(),\n",
    "        'shape': vector.shape\n",
    "    }\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(entry, f)\n",
    "\n",
    "# Main program\n",
    "documents = load_jsonl('./data/documents' + DataNum + '.jsonl')\n",
    "questions = load_jsonl('./data/train' + DataNum + '.jsonl')\n",
    "\n",
    "# Preprocess documents and questions\n",
    "doc_dict = preprocess_documents(documents)\n",
    "vectorizer = TfidfVectorizer(tokenizer=segment_text)\n",
    "all_text = list(doc_dict.values()) + [q['question'] for q in questions]\n",
    "tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "\n",
    "# Split TF-IDF matrix into documents and questions\n",
    "num_doc = len(doc_dict)\n",
    "doc_tfidf_matrix = tfidf_matrix[:num_doc]\n",
    "question_tfidf_matrix = tfidf_matrix[num_doc:]\n",
    "\n",
    "# Process questions to include TF-IDF vectors\n",
    "question_dict = preprocess_questions(questions, question_tfidf_matrix)\n",
    "\n",
    "# Create directory for vector files if it doesn't exist\n",
    "vector_dir = f'./data/TF_IDF/vector{DataNum}'\n",
    "os.makedirs(vector_dir, exist_ok=True)\n",
    "\n",
    "# Save document vectors to JSONL with optimized storage\n",
    "save_doc_vector_path = f'./data/TF_IDF/doc_vector{DataNum}.jsonl'\n",
    "with open(save_doc_vector_path, 'w', encoding='utf-8') as doc_file:\n",
    "    for i, (doc_id, doc_text) in enumerate(doc_dict.items()):\n",
    "        doc_vector = doc_tfidf_matrix[i]  # Access the sparse matrix using index\n",
    "        vector_file_path = f'{vector_dir}/vector_{doc_id}.json'\n",
    "        save_sparse_vector(doc_vector, vector_file_path)\n",
    "        doc_entry = {\n",
    "            'document_id': doc_id,\n",
    "            'document_text': doc_text,\n",
    "            'document_vector_path': vector_file_path  # Store path to vector file\n",
    "        }\n",
    "        doc_file.write(json.dumps(doc_entry) + '\\n')\n",
    "\n",
    "# Save question vectors to JSONL with optimized storage\n",
    "save_ques_vector_path = f'./data/TF_IDF/ques_vector{DataNum}.jsonl'\n",
    "with open(save_ques_vector_path, 'w', encoding='utf-8') as ques_file:\n",
    "    for question_text, question_data in question_dict.items():\n",
    "        vector_file_path = f'{vector_dir}/vector_ques{hash(question_text)}.json'\n",
    "        save_sparse_vector(question_data['vector'], vector_file_path)\n",
    "        ques_entry = {\n",
    "            'question_text': question_text,\n",
    "            'question_answer': question_data['answer'],\n",
    "            'document_id_answer': question_data['document_id'],\n",
    "            'question_vector_path': vector_file_path  # Store path to vector file\n",
    "        }\n",
    "        ques_file.write(json.dumps(ques_entry) + '\\n')\n",
    "\n",
    "# Validate accuracy\n",
    "accuracy = validate_accuracy(question_dict, doc_dict, doc_tfidf_matrix)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
