{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     g:\\Anaconda\\Miniconda\\envs\\MyNLP\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     g:\\Anaconda\\Miniconda\\envs\\MyNLP\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3353.3014202006157\n",
      "1432.5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 下载nltk的停用词数据\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 加载停用词\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"加载JSONL文件并返回字典列表。\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "    \"\"\"将文档信息存储到字典中。\"\"\"\n",
    "    doc_dict = {}\n",
    "    for doc in documents:\n",
    "        doc_id = doc['document_id']\n",
    "        doc_text = doc['document_text']\n",
    "        doc_dict[doc_id] = doc_text\n",
    "    return doc_dict\n",
    "\n",
    "def preprocess_questions(questions):\n",
    "    \"\"\"将问题、答案和参考文档ID存储到字典中。\"\"\"\n",
    "    question_dict = {}\n",
    "    for question in questions:\n",
    "        question_text = question['question']\n",
    "        answer_text = question['answer']\n",
    "        reference_doc_ids = question.get('document_id', [])\n",
    "        question_dict[question_text] = {\n",
    "            'answer': answer_text,\n",
    "            'document_id': reference_doc_ids\n",
    "        }\n",
    "    return question_dict\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"清洗文本 去掉HTML标签并转换为小写。\"\"\"\n",
    "    text = text.replace(\"'\", \"\")  \n",
    "    text = re.sub(r'<.*?>', ' ', text)  # 去掉所有的HTML标签\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 合并多个空格为一个空格\n",
    "    return text.lower()  # 转换为小写\n",
    "\n",
    "# 加载文档和问题\n",
    "documents = load_jsonl('./data/documents.jsonl')\n",
    "questions = load_jsonl('./data/train.jsonl')\n",
    "\n",
    "# 处理文档和问题\n",
    "doc_dict = preprocess_documents(documents)\n",
    "question_dict = preprocess_questions(questions)\n",
    "\n",
    "### Step 2: 分词并去掉停用词\n",
    "def segment_text(text):\n",
    "    \"\"\"对文本进行分句和分词，并去掉停用词。\"\"\"\n",
    "    cleaned_text = clean_text(text)  # 清洗文本\n",
    "    sentences = sent_tokenize(cleaned_text)  # 按句子分割\n",
    "    # print (sentences)\n",
    "    segmented_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words if ((word.lower() not in stop_words) or not word.isalpha())]\n",
    "        segmented_sentences.extend(filtered_words)\n",
    "    # print (segmented_sentences)\n",
    "    return segmented_sentences\n",
    "\n",
    "def segment_text(text):\n",
    "    \"\"\"对文本进行分词并去掉停用词。\"\"\"\n",
    "    cleaned_text = clean_text(text)  # 清洗文本\n",
    "    words = nltk.word_tokenize(cleaned_text)\n",
    "    return [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# 准备训练数据\n",
    "all_texts = list(doc_dict.values()) + list(question_dict.keys())\n",
    "segmented_texts = [segment_text(text) for text in all_texts]\n",
    "# segmented_texts = [sentence for text in all_texts for sentence in segment_text(text)]\n",
    "\n",
    "# 输出平均长度 中位数长度\n",
    "print(np.mean([len(text) for text in segmented_texts]))\n",
    "print(np.median([len(text) for text in segmented_texts]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# 训练 Word2Vec 模型\n",
    "word2vec_model = Word2Vec(sentences=segmented_texts, vector_size=1500, window=800, min_count=10, workers=32)\n",
    "# word2vec_model = Word2Vec(sentences=segmented_texts, vector_size=100, window=50, min_count=10, workers=32)\n",
    "\n",
    "# 获取文本的向量表示\n",
    "def get_text_vector(text, model):\n",
    "    \"\"\"计算给定文本的向量表示。\"\"\"\n",
    "    words = segment_text(text)\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# 计算文档的向量\n",
    "doc_vectors = {doc_id: get_text_vector(doc_text, word2vec_model) for doc_id, doc_text in doc_dict.items()}\n",
    "\n",
    "# 计算问题的向量\n",
    "question_vectors = {question_text: {\n",
    "    'vector': get_text_vector(question_text, word2vec_model),\n",
    "    'answer': question_dict[question_text]['answer'],\n",
    "    'document_id': question_dict[question_text]['document_id']\n",
    "} for question_text in question_dict.keys()}\n",
    "\n",
    "# 示例：计算每个问题与文档之间的余弦相似度\n",
    "tot_num = 0\n",
    "acc_num = 0\n",
    "\n",
    "for question_text, question_data in question_vectors.items():\n",
    "    question_vector = question_data['vector'].reshape(1, -1)  # 确保是二维数组\n",
    "    doc_vectors_array = np.array(list(doc_vectors.values()))  # 转换为数组\n",
    "\n",
    "    # 计算相似度\n",
    "    similarities = cosine_similarity(question_vector, doc_vectors_array)\n",
    "    \n",
    "    # 找到最匹配的五个文档\n",
    "    top_5_indices = similarities.argsort()[0][-5:][::-1]\n",
    "    \n",
    "    # 根据相对index找到doc_id\n",
    "    top_5_doc_ids = [list(doc_dict.keys())[i] for i in top_5_indices]\n",
    "    \n",
    "    # 输出结果\n",
    "    print(f\"Question: {question_text}\")\n",
    "    print(f\"Answer: {question_data['answer']}\")\n",
    "    print(f\"Reference Document ID: {question_data['document_id']}\")\n",
    "    print(\"Top 5 Similar Documents:\")\n",
    "    print(top_5_doc_ids)\n",
    "    \n",
    "    tot_num += 1\n",
    "    if question_data['document_id'] in top_5_doc_ids:\n",
    "        acc_num += 1\n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Accuracy:\", acc_num / tot_num)\n",
    "# '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
