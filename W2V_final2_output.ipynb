{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 12138/12138 [00:00<00:00, 1734361.99it/s]\n",
      "Tokenizing documents: 100%|██████████| 12138/12138 [07:56<00:00, 25.48it/s]\n",
      "Processing questions: 100%|██████████| 8000/8000 [00:01<00:00, 6364.25it/s]\n",
      "Generating document vectors: 100%|██████████| 12138/12138 [15:35<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved to ./model/W2V_1400_1000_10.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating accuracy: 100%|██████████| 8000/8000 [00:00<00:00, 2000502.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.606625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 加载停用词\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Vector_size = 1400\n",
    "Window_size = 1000\n",
    "Min_Count = 10\n",
    "DataNum = ''\n",
    "Dir = './data/W2V/'\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"加载 JSONL 文件并返回字典列表。\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"清洗文本，去掉 HTML 标签并转换为小写。\"\"\"\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # 去掉所有 HTML 标签\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 合并多个空格为一个空格\n",
    "    return text.lower()  # 转换为小写\n",
    "\n",
    "def segment_text(text):\n",
    "    \"\"\"对文本进行分词并去掉停用词。\"\"\"\n",
    "    cleaned_text = clean_text(text)\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "    \"\"\"将文档信息存储到字典中。\"\"\"\n",
    "    doc_dict = {}\n",
    "    for doc in tqdm(documents, desc=\"Processing documents\"):\n",
    "        doc_id = doc['document_id']\n",
    "        doc_text = doc['document_text']\n",
    "        doc_dict[doc_id] = doc_text\n",
    "    return doc_dict\n",
    "\n",
    "def get_vector_mean(model, words):\n",
    "    \"\"\"获取词向量的均值，忽略不在词汇表中的词。\"\"\"\n",
    "    valid_words = [word for word in words if word in model.wv]\n",
    "    if not valid_words:\n",
    "        return None  # 如果没有有效词，则返回 None\n",
    "    return torch.mean(torch.tensor(model.wv[valid_words]), dim=0).unsqueeze(0)\n",
    "\n",
    "def preprocess_questions(questions, model):\n",
    "    \"\"\"将问题、答案和参考文档ID存储到字典中，同时生成问题向量。\"\"\"\n",
    "    question_dict = {}\n",
    "    for question in tqdm(questions, desc=\"Processing questions\"):\n",
    "        question_text = question['question']\n",
    "        answer_text = question['answer']\n",
    "        reference_doc_ids = question.get('document_id', [])\n",
    "        \n",
    "        # 计算问题向量\n",
    "        question_vector = get_vector_mean(model, segment_text(question_text))\n",
    "        \n",
    "        question_dict[question_text] = {\n",
    "            'answer': answer_text,\n",
    "            'document_id': reference_doc_ids,\n",
    "            'vector': question_vector  # 存储问题向量\n",
    "        }\n",
    "    return question_dict\n",
    "\n",
    "def train_word2vec(documents):\n",
    "    \"\"\"训练 Word2Vec 模型，并显示进度。\"\"\"\n",
    "    tokenized_docs = []\n",
    "    for doc in tqdm(documents.values(), desc=\"Tokenizing documents\"):\n",
    "        tokenized_docs.append(segment_text(doc))\n",
    "    \n",
    "    model = Word2Vec(sentences=tokenized_docs, vector_size=Vector_size, window=Window_size, min_count=Min_Count, workers=32)\n",
    "    return model\n",
    "\n",
    "def Candidate_Calculation(question_vector, doc_dict):\n",
    "    # 将文档向量转换为 NumPy 数组\n",
    "    doc_vectors_array = torch.stack(list(doc_vectors.values())).numpy()\n",
    "    doc_vectors_array = doc_vectors_array.squeeze()  # 去掉多余的维度\n",
    "\n",
    "    # for question_text, question_data in tqdm(question_vectors.items(), desc=\"Validating accuracy\"):\n",
    "    #     question_vector = question_data['vector'].detach().numpy()  # 获取问题向量\n",
    "\n",
    "    # 计算相似度\n",
    "    similarities = cosine_similarity(question_vector, doc_vectors_array)\n",
    "    \n",
    "    # 找到最匹配的五个文档\n",
    "    top_5_indices = similarities.argsort()[0][-5:][::-1]\n",
    "    \n",
    "    # 根据相对index找到doc_id\n",
    "    top_5_doc_ids = [list(doc_dict.keys())[i] for i in top_5_indices]\n",
    "    \n",
    "    # 输出结果\n",
    "    return top_5_doc_ids\n",
    "\n",
    "def validate_accuracy(question_dict):\n",
    "    # 判断 ['document_id_top5'] 是否包含 ['document_id_answer']\n",
    "    correct = 0\n",
    "    for question_text, question_data in tqdm(question_dict.items(), desc=\"Validating accuracy\"):\n",
    "        if question_data['document_id_answer'] in question_data['document_id_top5']:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(question_dict) if question_dict else 0\n",
    "    return accuracy\n",
    "\n",
    "# 主程序\n",
    "documents = load_jsonl('./data/documents'+DataNum+'.jsonl')\n",
    "questions = load_jsonl('./data/train'+DataNum+'.jsonl')\n",
    "\n",
    "# 预处理文档和问题\n",
    "doc_dict = preprocess_documents(documents)\n",
    "model = train_word2vec(doc_dict)\n",
    "question_dict = preprocess_questions(questions, model)\n",
    "\n",
    "# 生成词语到词向量的映射\n",
    "word_translation_mapping = {word: word for word in model.wv.index_to_key}\n",
    "\n",
    "# 生成文档向量\n",
    "doc_vectors = {}\n",
    "for doc_id, doc_text in tqdm(doc_dict.items(), desc=\"Generating document vectors\"):\n",
    "    doc_vec_mean = get_vector_mean(model, segment_text(doc_text))\n",
    "    if doc_vec_mean is not None:\n",
    "        doc_vectors[doc_id] = doc_vec_mean\n",
    "\n",
    "save_model_path = './model/W2V'+ str(DataNum) + '_' + str(Vector_size) + '_' + str(Window_size) + '_' + str(Min_Count)+'.model'\n",
    "model.save(save_model_path)\n",
    "print(f\"Word2Vec model saved to {save_model_path}\")\n",
    "save_doc_vector_path = '{Dir}doc_vector'+ str(DataNum) + '_' + str(Vector_size) + '_' + str(Window_size) + '_' + str(Min_Count) + '.jsonl'\n",
    "save_ques_vector_path = '{Dir}ques_vector'+ str(DataNum) + '_' + str(Vector_size) + '_' + str(Window_size) + '_' + str(Min_Count) + '.jsonl'\n",
    "\n",
    "# 保存文档向量到 JSONL\n",
    "with open(save_doc_vector_path, 'w', encoding='utf-8') as doc_file:\n",
    "    for doc_id, doc_text in doc_dict.items():\n",
    "        doc_vector = doc_vectors.get(doc_id).detach().numpy().tolist() if doc_id in doc_vectors else None\n",
    "        doc_entry = {\n",
    "            'document_id': doc_id,\n",
    "            'document_text': doc_text,\n",
    "            'document_vector': doc_vector\n",
    "        }\n",
    "        doc_file.write(json.dumps(doc_entry) + '\\n')\n",
    "\n",
    "# 保存问题向量到 JSONL，并将整个字典保存到变量 val_dict\n",
    "# dictionary \n",
    "val_dict = {}\n",
    "with open(save_ques_vector_path, 'w', encoding='utf-8') as ques_file:\n",
    "    for question_text, question_data in question_dict.items():\n",
    "        ques_entry = {\n",
    "            'question_text': question_text,\n",
    "            'question_answer': question_data['answer'],\n",
    "            'document_id_answer': question_data['document_id'],\n",
    "            'document_id_top5': Candidate_Calculation(question_data['vector'].detach().numpy(), doc_dict),\n",
    "            'question_vector': question_data['vector'].detach().numpy().tolist() if question_data['vector'] is not None else None\n",
    "        }\n",
    "        val_dict[question_text] = ques_entry\n",
    "        ques_file.write(json.dumps(ques_entry) + '\\n')\n",
    "# 保存单词翻译表到 JSONL\n",
    "translation_path = '{Dir}word_translation.jsonl'\n",
    "with open(translation_path, 'w', encoding='utf-8') as trans_file:\n",
    "    for word, translation in word_translation_mapping.items():\n",
    "        translation_entry = {\n",
    "            'word': word,\n",
    "            'translation': translation  # 将其替换为实际翻译\n",
    "        }\n",
    "        trans_file.write(json.dumps(translation_entry) + '\\n')\n",
    "# 验证准确性\n",
    "print(validate_accuracy(val_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_translation(file_path):\n",
    "    \"\"\"加载单词翻译表并返回字典。\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return {json.loads(line)['word']: json.loads(line)['translation'] for line in file}\n",
    "def load_model(file_path):\n",
    "    \"\"\"加载 Word2Vec 模型。\"\"\"\n",
    "    return Word2Vec.load(file_path)\n",
    "def print_vocabulary(model):\n",
    "    \"\"\"打印模型的词汇表。\"\"\"\n",
    "    vocab = model.wv.key_to_index  # 获取词汇表\n",
    "    print(\"模型词汇表中的单词数量:\", len(vocab))\n",
    "    print(\"词汇表中的单词:\")\n",
    "    for word in vocab.keys():\n",
    "        print(word)\n",
    "def translate_text(vectors, model, translation_mapping, top_n=5):\n",
    "    \"\"\"\n",
    "    根据给定的词向量找到最相近的单词，并返回其翻译。\n",
    "    \"\"\"\n",
    "    translated_words = []\n",
    "    try:\n",
    "        # 找到与给定向量最相近的单词\n",
    "        for vector in vectors:\n",
    "            # 确保向量是有效的\n",
    "            if vector is not None and len(vector) == model.vector_size:\n",
    "                similar_words = model.wv.similar_by_vector(vector, topn=top_n)\n",
    "\n",
    "                # 获取翻译\n",
    "                for word, _ in similar_words:\n",
    "                    translation = translation_mapping.get(word, word)  # 查找翻译\n",
    "                    translated_words.append(translation)\n",
    "            else:\n",
    "                print(\"无效的向量，跳过该向量。\")\n",
    "        \n",
    "        return ' '.join(translated_words)  # 返回翻译后的文本\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"没有找到相似的单词，返回空字符串。\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"发生错误: {e}\")\n",
    "        return \"\"\n",
    "def calculate_accuracy(original_texts, translated_texts):\n",
    "    \"\"\"计算翻译的正确率，简单地比较原文与翻译的相等性。\"\"\"\n",
    "    correct_count = sum(1 for orig, trans in zip(original_texts, translated_texts) if orig == trans)\n",
    "    return correct_count / len(original_texts) if original_texts else 0\n",
    "\n",
    "# 加载单词翻译表\n",
    "translation_mapping = load_word_translation(translation_path)\n",
    "\n",
    "# 翻译文档并计算正确率\n",
    "original_texts = []\n",
    "original_vectors = []\n",
    "translated_texts = []\n",
    "\n",
    "model = load_model(save_model_path)\n",
    "with open(save_doc_vector_path, 'r', encoding='utf-8') as doc_file:\n",
    "    for line in doc_file:\n",
    "        doc_entry = json.loads(line)\n",
    "        doc_id = doc_entry['document_id']\n",
    "        doc_text = doc_entry['document_text']\n",
    "        doc_vector = doc_entry['document_vector']\n",
    "\n",
    "        # 保存原始文本\n",
    "        original_texts.append(doc_text)\n",
    "\n",
    "        # 翻译文档文本\n",
    "        translated_text = translate_text(doc_vector,model, translation_mapping)\n",
    "        translated_texts.append(translated_text)\n",
    "\n",
    "        original_vectors.append(doc_vector)\n",
    "        print(doc_vector)\n",
    "\n",
    "        # 这里可以选择保存翻译后的文档\n",
    "        # 例如：保存到一个文件或其他数据结构\n",
    "\n",
    "# 计算翻译的正确率\n",
    "print(original_texts)\n",
    "print(translated_texts)\n",
    "# print_vocabulary(model)\n",
    "# print(original_vectors)\n",
    "accuracy = calculate_accuracy(original_texts, translated_texts)\n",
    "\n",
    "print(f\"翻译的正确率是: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
